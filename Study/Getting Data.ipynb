{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_domain(email_address):\n",
    "    return email_address.lower().split('@')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Danniel\\\\Desktop\\\\DVC 2018 Winter\\\\email_addresses.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-61f71baf82a1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'C:\\\\Users\\Danniel\\\\Desktop\\\\DVC 2018 Winter\\\\email_addresses.txt'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     domain_counts = Counter(get_domain(line.strip())\n\u001b[0;32m      3\u001b[0m                            \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                            if '@' in line)\n\u001b[0;32m      5\u001b[0m \u001b[0mdomain_counts\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\Danniel\\\\Desktop\\\\DVC 2018 Winter\\\\email_addresses.txt'"
     ]
    }
   ],
   "source": [
    "with open('C:\\\\Users\\Danniel\\\\Desktop\\\\DVC 2018 Winter\\\\email_addresses.txt','r') as f:\n",
    "    domain_counts = Counter(get_domain(line.strip())\n",
    "                           for line in f\n",
    "                           if '@' in line)\n",
    "domain_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = requests.get('http://www.example.com').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html,'html5lib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_paragraph = soup.find('p')\n",
    "first_paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_paragraph_text = soup.p.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.safaribooksonline.com/search/?query=data&extended_publisher_data=true&highlight=true&include_assessments=false&include_case_studies=true&include_courses=true&include_orioles=true&include_playlists=true&is_academic_institution_account=false&sort=relevance'\n",
    "soup = BeautifulSoup(requests.get(url).text, 'html5lib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h3s = [span\n",
    "      for div in soup('div')\n",
    "      for span in div('span')]\n",
    "h3s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_book = div.find('span')\n",
    "first_book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'Data Sceience Book', 'author': 'Joel Grus', 'publicationYear': 2014, 'topics': ['data', 'science', 'data science']}\n"
     ]
    }
   ],
   "source": [
    "serialized = \"\"\"{\"title\": \"Data Sceience Book\",\n",
    "                \"author\": \"Joel Grus\",\n",
    "                \"publicationYear\": 2014,\n",
    "                \"topics\": [\"data\",\"science\",\"data science\"]}\"\"\"\n",
    "\n",
    "deserialized = json.loads(serialized)\n",
    "if 'data science' in deserialized['topics']:\n",
    "    print (deserialized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "endPoint = \"https://api.github.com/users/danniely/repos\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "repos = json.loads(requests.get(endPoint).text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil.parser import parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = [parse(repo[\"created_at\"]) for repo in repos]\n",
    "month_counts = Counter(date.month for date in dates)\n",
    "weekday_counts = Counter(date.weekday() for date in dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 1})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "month_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'Jupyter Notebook': 1})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_language = Counter(repo[\"language\"] for repo in repos)\n",
    "user_language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from twython import Twython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONSUMER_KEY = 'nHmxXWHNdl7HQqoxBsos3WU5D'\n",
    "CONSUMER_SECRET = 'VqXzXU6BqR1HKNHjPq9VAvoex7qI1xuJHyh3uVNJFwZiy9wVSY'\n",
    "ACCESS_TOKEN = '1084990784162521088-VQiEswpxWclrwD22kZo3q7nKRV2O2J'\n",
    "ACCESS_TOKEN_SECRET = '5sydqd69fQWwqGOMHvlGhmI3QLCK7pZRMjYaVJL2wrSdX'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter = Twython(CONSUMER_KEY, CONSUMER_SECRET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ahmedjr_16 : 45 Best #DataScience Courses For #DataScientist\n",
      "\n",
      "#AI #ArtificialIntelligence #BI #MachineLearningâ€¦ https://t.co/Y3fdDWkzHK\n",
      "ItunuTaylor : RT @MomentsWithBren: arise: 2019 Data Science Internship\n",
      "\n",
      "Start your data science career with Paylater! The arise program provides an excelâ€¦\n",
      "Visiola_Fdn : RT @paylaterNG: We're happy to announce the Arise 2019 Data Science Internship by Paylater ðŸŽ‰.\n",
      "\n",
      "If you want to advance your skills working oâ€¦\n",
      "drnjshelton : RT @JulieGeorgePH: interview date for Public Health Data Science fixed term post now confirmed for 23rd January. Please get in touch if youâ€¦\n",
      "azharsketch : RT @WeAreRLadies: Since I'm a complete pack rat, I'll now share with you a bunch of great, free online books to help learn #rstats.  \n",
      "1) Thâ€¦\n",
      "hellopurpleic : Ten Myths About Data Science https://t.co/pUOSkwWSok\n",
      "GengoIt : RT @SASGreece: Discover more about how Machine Learning is transforming financial services in Gengo's latest interview with SAS' Head of Daâ€¦\n",
      "TechnoJeder : RT @anitayorker: Can Artificial Intelligence replace Data Scientists? - Data Science Central https://t.co/gD8B82iJ9D, see more https://t.coâ€¦\n",
      "AstroLabsME : ðŸš¦A hands-on workshop in Deep Learning, Machine Learning, AI, Data Science, and so much more! ðŸš€ðŸš€ðŸš€No abstract conceptâ€¦ https://t.co/xzRiuMwJ4I\n",
      "anitayorker : Can Artificial Intelligence replace Data Scientists? - Data Science Central https://t.co/gD8B82iJ9D, see more https://t.co/L1rR6e0Ija\n",
      "ColinQuirke : @JoeSmyth10 @sarahcareyIRL Finland are preparing for the future - https://t.co/jmyv3dFafw\n",
      "JacquiMarshall2 : RT @xtaldave: Important Data Science thread! https://t.co/7Zypa1ELCH\n",
      "eolutosin : arise: 2019 Data Science Internship | https://t.co/BUE7BOtOye https://t.co/1T5xG41hb2\n",
      "Steven_Ramage : @edzerpebesma and @RogerBivand are writing a book on new stuff in #rspatial, called \"Spatial Data Science\". The firâ€¦ https://t.co/6ncwRIjpAZ\n",
      "Nova_Quality : No busques mÃ¡s, en @Nova_Quality queremos contar con un Consultor Experto en DATA SCIENCE. EnvÃ­anos tu currÃ­culum aâ€¦ https://t.co/OayYa1nMXU\n"
     ]
    }
   ],
   "source": [
    "for status in twitter.search(q='\"data science\"')['statuses']:\n",
    "    user = status['user']['screen_name']\n",
    "    text = status['text']\n",
    "    print (user, ':', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from twython import TwythonStreamer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyStreamer(TwythonStreamer):\n",
    "    def on_success(self, data):\n",
    "        if data['lang'] == 'en':\n",
    "            tweets.append(data)\n",
    "            print ('received tweet #', len(tweets))\n",
    "            \n",
    "        if len(tweets) >= 100:\n",
    "            self.disconnect()\n",
    "        \n",
    "    def on_error(self, status_code, data):\n",
    "        print (status_code, data)\n",
    "        self.disconnect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = MyStreamer(CONSUMER_KEY, CONSUMER_SECRET, ACCESS_TOKEN, ACCESS_TOKEN_SECRET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "received tweet # 1\n",
      "received tweet # 2\n",
      "received tweet # 3\n",
      "received tweet # 4\n",
      "received tweet # 5\n",
      "received tweet # 6\n",
      "received tweet # 7\n",
      "received tweet # 8\n",
      "received tweet # 9\n",
      "received tweet # 10\n",
      "received tweet # 11\n",
      "received tweet # 12\n",
      "received tweet # 13\n",
      "received tweet # 14\n",
      "received tweet # 15\n",
      "received tweet # 16\n",
      "received tweet # 17\n",
      "received tweet # 18\n",
      "received tweet # 19\n",
      "received tweet # 20\n",
      "received tweet # 21\n",
      "received tweet # 22\n",
      "received tweet # 23\n",
      "received tweet # 24\n",
      "received tweet # 25\n",
      "received tweet # 26\n",
      "received tweet # 27\n",
      "received tweet # 28\n",
      "received tweet # 29\n",
      "received tweet # 30\n",
      "received tweet # 31\n",
      "received tweet # 32\n",
      "received tweet # 33\n",
      "received tweet # 34\n",
      "received tweet # 35\n",
      "received tweet # 36\n",
      "received tweet # 37\n",
      "received tweet # 38\n",
      "received tweet # 39\n",
      "received tweet # 40\n",
      "received tweet # 41\n",
      "received tweet # 42\n",
      "received tweet # 43\n",
      "received tweet # 44\n",
      "received tweet # 45\n",
      "received tweet # 46\n",
      "received tweet # 47\n",
      "received tweet # 48\n",
      "received tweet # 49\n",
      "received tweet # 50\n",
      "received tweet # 51\n",
      "received tweet # 52\n",
      "received tweet # 53\n",
      "received tweet # 54\n",
      "received tweet # 55\n",
      "received tweet # 56\n",
      "received tweet # 57\n",
      "received tweet # 58\n",
      "received tweet # 59\n",
      "received tweet # 60\n",
      "received tweet # 61\n",
      "received tweet # 62\n",
      "received tweet # 63\n",
      "received tweet # 64\n",
      "received tweet # 65\n",
      "received tweet # 66\n",
      "received tweet # 67\n",
      "received tweet # 68\n",
      "received tweet # 69\n",
      "received tweet # 70\n",
      "received tweet # 71\n",
      "received tweet # 72\n",
      "received tweet # 73\n",
      "received tweet # 74\n",
      "received tweet # 75\n",
      "received tweet # 76\n",
      "received tweet # 77\n",
      "received tweet # 78\n",
      "received tweet # 79\n",
      "received tweet # 80\n",
      "received tweet # 81\n",
      "received tweet # 82\n",
      "received tweet # 83\n",
      "received tweet # 84\n",
      "received tweet # 85\n",
      "received tweet # 86\n",
      "received tweet # 87\n",
      "received tweet # 88\n",
      "received tweet # 89\n",
      "received tweet # 90\n",
      "received tweet # 91\n",
      "received tweet # 92\n",
      "received tweet # 93\n",
      "received tweet # 94\n",
      "received tweet # 95\n",
      "received tweet # 96\n",
      "received tweet # 97\n",
      "received tweet # 98\n",
      "received tweet # 99\n",
      "received tweet # 100\n"
     ]
    }
   ],
   "source": [
    "stream.statuses.filter(track='data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('data', 4), ('ai', 3), ('brexit', 3), ('tech', 3), ('blockchain', 2)]\n"
     ]
    }
   ],
   "source": [
    "top_hashtags = Counter(hashtag['text'].lower()\n",
    "                      for tweet in tweets\n",
    "                      for hashtag in tweet['entities']['hashtags'])\n",
    "print (top_hashtags.most_common(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
